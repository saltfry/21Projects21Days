{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Super-Resolution using U-Net\n\nThis notebook demonstrates how to build and train a U-Net model for image super-resolution.  \nThe goal is to take a low-resolution image (64x64) and generate a high-resolution version (128x128).\n\nThe U-Net architecture is well-suited for this task as it effectively captures both local and global features through its encoder-decoder structure with skip connections.\n\n![Super Resolution Example](https://raw.githubusercontent.com/AshishJangra27/ai-projects/refs/heads/main/Image%20Enhancement%20with%20U-Net/img.png)\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Setup and Imports\n\nThis cell imports all the necessary libraries for building and training the U-Net model, including Keras for model definition, OpenCV for image processing, and Matplotlib for visualization.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2 as cv\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, BatchNormalization, Activation, Dense, Dropout,\n    Conv2D, Conv2DTranspose, MaxPooling2D, GlobalMaxPool2D, concatenate\n)\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T10:30:29.321551Z","iopub.execute_input":"2026-01-07T10:30:29.321829Z","iopub.status.idle":"2026-01-07T10:30:43.666657Z","shell.execute_reply.started":"2026-01-07T10:30:29.321796Z","shell.execute_reply":"2026-01-07T10:30:43.666028Z"}},"outputs":[{"name":"stderr","text":"2026-01-07 10:30:31.103996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767781831.263955      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767781831.313811      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767781831.690045      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767781831.690098      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767781831.690104      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767781831.690109      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 2. U-Net Model Definition\n\nThis cell defines the U-Net model architecture for image super-resolution. The `unet_64to128` function creates a U-Net model that takes a 64x64x3 image as input and outputs a 128x128x3 image. It includes an encoder, a bottleneck, and a decoder with skip connections.","metadata":{}},{"cell_type":"code","source":"def unet_32to128(input_shape=(32, 32, 3), n_classes=3, final_activation='sigmoid', dropout_rate=0.05):\n    inputs = Input(shape=input_shape, name='img')\n\n    # Encoder\n    c1 = Conv2D(64, (3, 3), padding='same')(inputs)\n    c1 = BatchNormalization()(c1)\n    c1 = Activation('relu')(c1)\n    c1 = Conv2D(64, (3, 3), padding='same')(c1)\n    c1 = BatchNormalization()(c1)\n    c1 = Activation('relu')(c1)\n    p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n    \n    c2 = Conv2D(32, (3,3), padding='same')(p1)\n    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n    c2 = Conv2D(32, (3,3), padding='same')(c2)\n    c2 = BatchNormalization()(c2); c2 = Activation('relu')(c2)\n    p2 = MaxPooling2D((2,2))(c2); p2 = Dropout(dropout_rate)(p2)   # 32 -> 16\n\n    c3 = Conv2D(64, (3,3), padding='same')(p2)\n    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n    c3 = Conv2D(64, (3,3), padding='same')(c3)\n    c3 = BatchNormalization()(c3); c3 = Activation('relu')(c3)\n    p3 = MaxPooling2D((2,2))(c3); p3 = Dropout(dropout_rate)(p3)   # 16 -> 8\n\n    c4 = Conv2D(128, (3,3), padding='same')(p3)\n    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n    c4 = Conv2D(128, (3,3), padding='same')(c4)\n    c4 = BatchNormalization()(c4); c4 = Activation('relu')(c4)\n    p4 = MaxPooling2D((2,2))(c4); p4 = Dropout(dropout_rate)(p4)   # 8 -> 4\n\n    # Bottleneck (4x4)\n    c5 = Conv2D(256, (3,3), padding='same')(p4)\n    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n    c5 = Conv2D(256, (3,3), padding='same')(c5)\n    c5 = BatchNormalization()(c5); c5 = Activation('relu')(c5)\n\n    # Decoder (back to 64x64)\n    u6 = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same')(c5)  # 4 -> 8\n    u6 = concatenate([u6, c4]); u6 = Dropout(dropout_rate)(u6)\n    u6 = Conv2D(128, (3,3), padding='same')(u6)\n    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n    u6 = Conv2D(128, (3,3), padding='same')(u6)\n    u6 = BatchNormalization()(u6); u6 = Activation('relu')(u6)\n\n    u7 = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same')(u6)    # 8 -> 16\n    u7 = concatenate([u7, c3]); u7 = Dropout(dropout_rate)(u7)\n    u7 = Conv2D(64, (3,3), padding='same')(u7)\n    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n    u7 = Conv2D(64, (3,3), padding='same')(u7)\n    u7 = BatchNormalization()(u7); u7 = Activation('relu')(u7)\n\n    u8 = Conv2DTranspose(32, (3,3), strides=(2,2), padding='same')(u7)    # 16 -> 32\n    u8 = concatenate([u8, c2]); u8 = Dropout(dropout_rate)(u8)\n    u8 = Conv2D(32, (3,3), padding='same')(u8)\n    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n    u8 = Conv2D(32, (3,3), padding='same')(u8)\n    u8 = BatchNormalization()(u8); u8 = Activation('relu')(u8)\n\n    u9 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u8)    # 32 -> 64\n    u9 = concatenate([u9, c1]); u9 = Dropout(dropout_rate)(u9)\n    u9 = Conv2D(16, (3,3), padding='same')(u9)\n    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n    u9 = Conv2D(16, (3,3), padding='same')(u9)\n    u9 = BatchNormalization()(u9); u9 = Activation('relu')(u9)\n\n    # Extra upsample to reach 128x128 (no skip)\n    u10 = Conv2DTranspose(16, (3,3), strides=(2,2), padding='same')(u9)   # 64 -> 128\n    u10 = Dropout(dropout_rate)(u10)\n    u10 = Conv2D(16, (3,3), padding='same')(u10)\n    u10 = BatchNormalization()(u10); u10 = Activation('relu')(u10)\n\n    outputs = Conv2D(n_classes, (1,1), activation=final_activation, name='mask')(u10)\n    \n    return Model(inputs=inputs, outputs=outputs, name='UNet_64to128')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2026-01-07T10:30:43.668510Z","iopub.execute_input":"2026-01-07T10:30:43.668999Z","iopub.status.idle":"2026-01-07T10:30:43.684733Z","shell.execute_reply.started":"2026-01-07T10:30:43.668974Z","shell.execute_reply":"2026-01-07T10:30:43.683981Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### 3. Model Initialization and Compilation\n\nThis cell initializes the U-Net model with the specified input shape and number of classes. It then compiles the model using the Adam optimizer and mean absolute error (MAE) as the loss function. The input and output shapes are printed to verify the model architecture.","metadata":{}},{"cell_type":"code","source":"model = unet_32to128(input_shape=(32,32,3), n_classes=3, final_activation='sigmoid')\n\nprint('Input shape:', model.input_shape)   # (None, 64, 64, 3)\nprint('Output shape:', model.output_shape) # (None, 128, 128, 3)\n\nmodel.compile(optimizer=Adam(1e-4), loss='mae' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T10:30:43.685826Z","iopub.execute_input":"2026-01-07T10:30:43.686157Z","iopub.status.idle":"2026-01-07T10:30:45.936078Z","shell.execute_reply.started":"2026-01-07T10:30:43.686126Z","shell.execute_reply":"2026-01-07T10:30:45.935298Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1767781844.329657      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Input shape: (None, 32, 32, 3)\nOutput shape: (None, 64, 64, 3)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### 4. Data Generator\n\nThis cell defines a data generator function `datagen` that loads and preprocesses images from the CelebA dataset. It resizes the images to 128x128 as ground truth and to 64x64 as low-resolution input, and normalizes the pixel values. The generator yields batches of low-resolution and high-resolution image pairs for training.","metadata":{}},{"cell_type":"code","source":"import os \nimport cv2 as cv\nimport numpy as np\n\n\nimgs = os.listdir('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/')\n\n\ndef datagen(batch_size):\n    \n    while True:\n        x_batch = []\n        y_batch = []\n        \n        for _ in range(batch_size):\n            indx = np.random.randint(0, len(imgs))\n            \n            bgr = cv.imread('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/' + imgs[indx])\n            bgr = cv.resize(bgr, (128, 128))\n            rgb = cv.cvtColor(bgr, cv.COLOR_BGR2RGB)\n\n            # blur = cv.blur(rgb, (4, 4))\n\n            x = cv.resize(rgb, (64, 64))\n            x = x / 255.0\n            y = rgb / 255.0\n\n            x_batch.append(x)\n            y_batch.append(y)\n        \n        x_batch = np.array(x_batch).reshape(batch_size, 32, 64, 3)\n        y_batch = np.array(y_batch).reshape(batch_size, 128, 128, 3)\n        \n        yield x_batch, y_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T10:30:45.937063Z","iopub.execute_input":"2026-01-07T10:30:45.937366Z","iopub.status.idle":"2026-01-07T10:30:45.948663Z","shell.execute_reply.started":"2026-01-07T10:30:45.937329Z","shell.execute_reply":"2026-01-07T10:30:45.947479Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/75202524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'","output_type":"error"}],"execution_count":4},{"cell_type":"markdown","source":"### 5. Model Training (Short Run)\n\nThis cell trains the U-Net model for a small number of epochs (5) using the `datagen` function. This is a short run to quickly check if the model is training without errors.","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\nbatch_size = 32\nnum_images = len(imgs)\nsteps_per_epoch = num_images // batch_size \n\n\nresults = model.fit(datagen(batch_size=batch_size), steps_per_epoch=steps_per_epoch , epochs=5, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T10:30:45.949307Z","iopub.status.idle":"2026-01-07T10:30:45.949574Z","shell.execute_reply.started":"2026-01-07T10:30:45.949436Z","shell.execute_reply":"2026-01-07T10:30:45.949455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. Model Training with Callbacks and Sample Saving\n\nThis cell trains the U-Net model for a longer duration (3 epochs) and includes custom callbacks. The `show_and_save_samples` function generates and saves sample super-resolved images at the end of each epoch, along with the original, ground truth, and low-resolution images for comparison. The `LambdaCallback` is used to execute this function after each epoch. The model checkpoints are also saved during training.","metadata":{}},{"cell_type":"code","source":"import os, cv2 as cv, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n\ndef show_and_save_samples(model, imgs, base_dir, epoch, n=5, save_dir='./epoch_samples'):\n    os.makedirs(save_dir, exist_ok=True)\n    chosen = np.random.choice(imgs, n, replace=False)\n    fig, axes = plt.subplots(n, 4, figsize=(16, 4*n))\n    if n == 1: axes = np.expand_dims(axes, 0)\n\n    for i, fname in enumerate(chosen):\n        path = os.path.join(base_dir, fname)\n        rgb = cv.cvtColor(cv.imread(path), cv.COLOR_BGR2RGB)\n        gt128 = cv.resize(rgb, (128,128)).astype('float32')/255\n        img64 = cv.resize(rgb, (32,32)).astype('float32')/255\n        pred  = model.predict(np.expand_dims(img64,0), verbose=0)[0]\n        lowup = cv.resize((img64*255).astype(np.uint8),(128,128))\n\n        for ax, im, title in zip(axes[i],[rgb,gt128,lowup,pred],\n                                 ['Original','GT 128x128','Low-Res','Prediction']):\n            ax.imshow(im); ax.set_title(title); ax.axis('off')\n\n    plt.tight_layout(); plt.savefig(f\"{save_dir}/epoch_{epoch+1:02d}.png\"); plt.show()\n\n# --- Simple Callbacks ---\ndef on_epoch_end(epoch, logs):\n    show_and_save_samples(model, imgs, base_dir, epoch, n=5)\n    model.save(f'./checkpoints/model_epoch_{epoch+1:02d}.keras')\n\nshow_cb = tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)\n\n\nbatch_size = 32\nsteps_per_epoch = len(imgs)//batch_size\nbase_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\nos.makedirs('./checkpoints', exist_ok=True)\n\nresults = model.fit(\n    datagen(batch_size=batch_size),\n    steps_per_epoch=steps_per_epoch,\n    epochs=3,\n    callbacks=[show_cb],\n    verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T10:30:45.950755Z","iopub.status.idle":"2026-01-07T10:30:45.951028Z","shell.execute_reply.started":"2026-01-07T10:30:45.950905Z","shell.execute_reply":"2026-01-07T10:30:45.950924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### 7. Project Summary\n\nThis notebook successfully implemented and trained a U-Net model for image super-resolution. The model was trained on the CelebA dataset, taking 64x64 images as input and generating 128x128 images.\n\nThe training process showed a decrease in the mean absolute error (MAE) loss over epochs, indicating that the model is learning to reconstruct higher-resolution images. The generated sample images at the end of each epoch provide a visual representation of the model's progress.\n\nFurther improvements could include:\n- Experimenting with different loss functions (e.g., perceptual loss)\n- Using a larger and more diverse dataset\n- Implementing more advanced super-resolution techniques (e.g., Generative Adversarial Networks)\n- Hyperparameter tuning to optimize model performance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T10:30:45.953153Z","iopub.status.idle":"2026-01-07T10:30:45.953531Z","shell.execute_reply.started":"2026-01-07T10:30:45.953341Z","shell.execute_reply":"2026-01-07T10:30:45.953362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}